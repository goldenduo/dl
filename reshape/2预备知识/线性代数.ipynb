{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2917a7",
   "metadata": {},
   "source": [
    "# 线性代数知识点整理 - PyTorch版本\n",
    "\n",
    "## 1. 标量 (Scalar)\n",
    "\n",
    "**概念**: 标量是只包含一个数值的数学对象，用普通小写字母表示（如x、y、z）。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "293b2f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:26:59.314157Z",
     "start_time": "2025-07-01T03:26:57.804633Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# 创建标量\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# 标量运算\n",
    "print(f\"加法: {x + y}\")      # tensor(5.)\n",
    "print(f\"乘法: {x * y}\")      # tensor(6.)\n",
    "print(f\"除法: {x / y}\")      # tensor(1.5000)\n",
    "print(f\"幂运算: {x ** y}\")    # tensor(9.)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加法: 5.0\n",
      "乘法: 6.0\n",
      "除法: 1.5\n",
      "幂运算: 9.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "992cb367",
   "metadata": {},
   "source": [
    "## 2. 向量 (Vector)\n",
    "\n",
    "**概念**: 向量是标量值组成的列表，用粗体小写字母表示（如**x**、**y**、**z**）。向量的元素称为分量。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c91ac614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:27:10.726908Z",
     "start_time": "2025-07-01T03:27:10.721108Z"
    }
   },
   "source": [
    "# 创建向量\n",
    "x = torch.arange(4)  # tensor([0, 1, 2, 3])\n",
    "print(f\"向量: {x}\")\n",
    "print(f\"访问第4个元素: {x[3]}\")      # tensor(3)\n",
    "print(f\"向量长度: {len(x)}\")          # 4\n",
    "print(f\"向量形状: {x.shape}\")         # torch.Size([4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量: tensor([0, 1, 2, 3])\n",
      "访问第4个元素: 3\n",
      "向量长度: 4\n",
      "向量形状: torch.Size([4])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "41e4fad1",
   "metadata": {},
   "source": [
    "## 3. 矩阵 (Matrix)\n",
    "\n",
    "**概念**: 矩阵是向量从一阶推广到二阶的数据结构，用粗体大写字母表示（如**A**、**B**、**C**）。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建矩阵\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "print(f\"矩阵A:\\n{A}\")\n",
    "print(f\"矩阵形状: {A.shape}\")        # torch.Size([5, 4])\n",
    "\n",
    "# 矩阵转置\n",
    "print(f\"矩阵转置:\\n{A.T}\")\n",
    "\n",
    "# 对称矩阵\n",
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "print(f\"B是否等于其转置: {torch.equal(B, B.T)}\")  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ce8b9",
   "metadata": {},
   "source": [
    "## 4. 张量 (Tensor)\n",
    "\n",
    "**概念**: 张量是具有任意数量轴的n维数组的通用表示，是标量、向量、矩阵的进一步推广。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建三阶张量\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"张量X:\\n{X}\")\n",
    "print(f\"张量形状: {X.shape}\")        # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db0394",
   "metadata": {},
   "source": [
    "## 5. 按元素运算\n",
    "\n",
    "**概念**: 相同形状的张量可以进行按元素的算术运算，结果张量形状保持不变。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 复制A\n",
    "\n",
    "# 按元素加法\n",
    "print(f\"按元素加法:\\n{A + B}\")\n",
    "\n",
    "# Hadamard积（按元素乘法）\n",
    "print(f\"Hadamard积:\\n{A * B}\")\n",
    "\n",
    "# 标量与张量运算\n",
    "a = 2\n",
    "print(f\"标量乘法形状: {(a * A).shape}\")  # torch.Size([5, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2c528",
   "metadata": {},
   "source": [
    "## 6. 求和与降维\n",
    "\n",
    "**概念**: 可以对张量的所有元素或沿指定轴进行求和操作，从而降低张量维度。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b168afd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:44:53.635594Z",
     "start_time": "2025-07-01T03:44:53.590109Z"
    }
   },
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "# 全部元素求和\n",
    "print(f\"所有元素求和: {A.sum()}\")           # tensor(190.)\n",
    "\n",
    "# 沿轴0求和（按列求和）\n",
    "print(f\"沿轴0求和: {A.sum(axis=0)}\")        # torch.Size([4])\n",
    "\n",
    "# 沿轴1求和（按行求和）\n",
    "print(f\"沿轴1求和: {A.sum(axis=1)}\")        # torch.Size([5])\n",
    "\n",
    "# 保持维度的求和\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "print(f\"保持维度求和形状: {sum_A.shape}\")    # torch.Size([5, 1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有元素求和: 190.0\n",
      "沿轴0求和: tensor([40., 45., 50., 55.])\n",
      "沿轴1求和: tensor([ 6., 22., 38., 54., 70.])\n",
      "保持维度求和形状: torch.Size([5, 1])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "13e65bf8",
   "metadata": {},
   "source": [
    "## 7. 平均值计算\n",
    "\n",
    "**概念**: 通过将总和除以元素总数来计算平均值，也可以沿指定轴计算。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "# 计算平均值\n",
    "print(f\"平均值: {A.mean()}\")                    # tensor(9.5000)\n",
    "print(f\"验证: {A.sum() / A.numel()}\")            # tensor(9.5000)\n",
    "\n",
    "# 沿轴计算平均值\n",
    "print(f\"沿轴0平均值: {A.mean(axis=0)}\")         # torch.Size([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36e83e",
   "metadata": {},
   "source": [
    "## 8. 累积求和\n",
    "\n",
    "**概念**: 沿某个轴计算元素的累积总和，不会降低张量维度。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e3b371f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:47:24.465781Z",
     "start_time": "2025-07-01T03:47:24.447761Z"
    }
   },
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "# 沿轴0累积求和\n",
    "cumsum_result = A.cumsum(axis=0)\n",
    "print(f\"累积求和:\\n{cumsum_result}\")\n",
    "print(f\"形状保持不变: {cumsum_result.shape}\")   # torch.Size([5, 4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积求和:\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n",
      "形状保持不变: torch.Size([5, 4])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "d088c3bc",
   "metadata": {},
   "source": [
    "## 9. 点积 (Dot Product)\n",
    "\n",
    "**概念**: 两个向量的点积是相同位置元素乘积的和，表示为x^T y。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 1, 2, 3], dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "\n",
    "# 点积计算\n",
    "dot_product = torch.dot(x, y)\n",
    "print(f\"点积: {dot_product}\")                    # tensor(6.)\n",
    "\n",
    "# 等价计算\n",
    "print(f\"等价计算: {torch.sum(x * y)}\")           # tensor(6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe25941",
   "metadata": {},
   "source": [
    "## 10. 矩阵-向量积\n",
    "\n",
    "**概念**: 矩阵A与向量x的乘积，结果是一个向量，每个元素是矩阵对应行与向量的点积。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a502023b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:50:01.029594Z",
     "start_time": "2025-07-01T03:50:01.001679Z"
    }
   },
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "\n",
    "# 矩阵-向量积\n",
    "result = torch.mv(A, x)\n",
    "print(f\"矩阵形状: {A.shape}\")        # torch.Size([5, 4])\n",
    "print(f\"向量形状: {x.shape}\")        # torch.Size([4])\n",
    "print(f\"结果形状: {result.shape}\")   # torch.Size([5])\n",
    "print(f\"结果: {result}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵形状: torch.Size([5, 4])\n",
      "向量形状: torch.Size([4])\n",
      "结果形状: torch.Size([5])\n",
      "结果: tensor([ 14.,  38.,  62.,  86., 110.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8c25d95f",
   "metadata": {},
   "source": [
    "## 11. 矩阵-矩阵乘法\n",
    "\n",
    "**概念**: 两个矩阵相乘，结果矩阵的每个元素是第一个矩阵对应行与第二个矩阵对应列的点积。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = torch.ones(4, 3)\n",
    "\n",
    "# 矩阵乘法\n",
    "C = torch.mm(A, B)\n",
    "print(f\"A形状: {A.shape}\")          # torch.Size([5, 4])\n",
    "print(f\"B形状: {B.shape}\")          # torch.Size([4, 3])\n",
    "print(f\"结果形状: {C.shape}\")       # torch.Size([5, 3])\n",
    "print(f\"结果:\\n{C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257a00b",
   "metadata": {},
   "source": [
    "## 12. L2范数\n",
    "\n",
    "**概念**: L2范数是向量元素平方和的平方根，表示向量的欧几里得长度。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f8904fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:53:37.400544Z",
     "start_time": "2025-07-01T03:53:37.395390Z"
    }
   },
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "\n",
    "# L2范数计算\n",
    "l2_norm = u.norm()\n",
    "print(f\"L2范数: {l2_norm}\")                      # tensor(5.)\n",
    "\n",
    "# 手动计算验证\n",
    "manual_l2 = torch.sqrt(torch.sum(u ** 2))\n",
    "print(f\"手动计算L2范数: {manual_l2}\")            # tensor(5.)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2范数: 5.0\n",
      "手动计算L2范数: 5.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "7335b29e",
   "metadata": {},
   "source": [
    "## 13. L1范数\n",
    "\n",
    "**概念**: L1范数是向量元素绝对值的和，相比L2范数受异常值影响较小。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "\n",
    "# L1范数计算\n",
    "l1_norm = torch.abs(u).sum()\n",
    "print(f\"L1范数: {l1_norm}\")                      # tensor(7.)\n",
    "\n",
    "# 使用norm函数计算L1范数\n",
    "l1_norm_func = torch.norm(u, p=1)\n",
    "print(f\"使用norm函数的L1范数: {l1_norm_func}\")   # tensor(7.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef996c1d",
   "metadata": {},
   "source": [
    "## 14. Frobenius范数\n",
    "\n",
    "**概念**: 矩阵的Frobenius范数是矩阵所有元素平方和的平方根，类似于矩阵的L2范数。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建4x9的全1矩阵\n",
    "matrix = torch.ones((4, 9))\n",
    "\n",
    "# Frobenius范数\n",
    "frobenius_norm = torch.norm(matrix)\n",
    "print(f\"Frobenius范数: {frobenius_norm}\")        # tensor(6.)\n",
    "\n",
    "# 手动计算验证\n",
    "manual_frob = torch.sqrt(torch.sum(matrix ** 2))\n",
    "print(f\"手动计算: {manual_frob}\")                # tensor(6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38426f26",
   "metadata": {},
   "source": [
    "## 15. 广播机制\n",
    "\n",
    "**概念**: 当张量形状兼容时，可以通过广播进行运算，较小的张量会被\"扩展\"以匹配较大张量的形状。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfc2eaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:24:56.869815Z",
     "start_time": "2025-07-01T06:24:56.841208Z"
    }
   },
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "sum_A = A.sum(axis=1, keepdims=True)  # 形状: [5, 1]\n",
    "\n",
    "# 广播除法\n",
    "result = A / sum_A\n",
    "print(f\"A:{A}\")\n",
    "print(f\"sumA:{sum_A}\")\n",
    "print(f\"result:{result}\")\n",
    "print(f\"A形状: {A.shape}\")              # torch.Size([5, 4])\n",
    "print(f\"sum_A形状: {sum_A.shape}\")      # torch.Size([5, 1])\n",
    "print(f\"广播结果形状: {result.shape}\")   # torch.Size([5, 4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "sumA:tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "result:tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n",
      "A形状: torch.Size([5, 4])\n",
      "sum_A形状: torch.Size([5, 1])\n",
      "广播结果形状: torch.Size([5, 4])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "04702c15",
   "metadata": {},
   "source": [
    "## 16. 张量索引和切片\n",
    "\n",
    "**概念**: 可以使用索引访问张量的特定元素，使用切片访问张量的子集。\n",
    "\n",
    "**例子**:"
   ]
  },
  {
   "cell_type": "code",
   "id": "11d3be31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:27:16.701672Z",
     "start_time": "2025-07-01T06:27:16.676528Z"
    }
   },
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "# 索引访问\n",
    "print(f\"访问[0,1,2]位置的元素: {X[0, 1, 2]}\")    # tensor(6)\n",
    "\n",
    "# 切片访问\n",
    "print(f\"第一个2D切片:\\n{X[0]}\")\n",
    "print(f\"所有第一列: {X[:, 0, :]}\")\n",
    "print(f\"形状: {X[:, 0, :].shape}\")              # torch.Size([2, 4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问[0,1,2]位置的元素: 6\n",
      "第一个2D切片:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "所有第一列: tensor([[ 0,  1,  2,  3],\n",
      "        [12, 13, 14, 15]])\n",
      "形状: torch.Size([2, 4])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "5df78ccd",
   "metadata": {},
   "source": [
    "这些知识点构成了深度学习中线性代数的基础，掌握这些操作对于理解和实现神经网络模型至关重要。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
