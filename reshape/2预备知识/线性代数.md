# 线性代数知识点整理 - PyTorch版本

## 1. 标量 (Scalar)

**概念**: 标量是只包含一个数值的数学对象，用普通小写字母表示（如x、y、z）。

**例子**:
```python
import torch

# 创建标量
x = torch.tensor(3.0)
y = torch.tensor(2.0)

# 标量运算
print(f"加法: {x + y}")      # tensor(5.)
print(f"乘法: {x * y}")      # tensor(6.)
print(f"除法: {x / y}")      # tensor(1.5000)
print(f"幂运算: {x ** y}")    # tensor(9.)
```

## 2. 向量 (Vector)

**概念**: 向量是标量值组成的列表，用粗体小写字母表示（如**x**、**y**、**z**）。向量的元素称为分量。

**例子**:
```python
# 创建向量
x = torch.arange(4)  # tensor([0, 1, 2, 3])
print(f"向量: {x}")
print(f"访问第4个元素: {x[3]}")      # tensor(3)
print(f"向量长度: {len(x)}")          # 4
print(f"向量形状: {x.shape}")         # torch.Size([4])
```

## 3. 矩阵 (Matrix)

**概念**: 矩阵是向量从一阶推广到二阶的数据结构，用粗体大写字母表示（如**A**、**B**、**C**）。

**例子**:
```python
# 创建矩阵
A = torch.arange(20).reshape(5, 4)
print(f"矩阵A:\n{A}")
print(f"矩阵形状: {A.shape}")        # torch.Size([5, 4])

# 矩阵转置
print(f"矩阵转置:\n{A.T}")

# 对称矩阵
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(f"B是否等于其转置: {torch.equal(B, B.T)}")  # True
```

## 4. 张量 (Tensor)

**概念**: 张量是具有任意数量轴的n维数组的通用表示，是标量、向量、矩阵的进一步推广。

**例子**:
```python
# 创建三阶张量
X = torch.arange(24).reshape(2, 3, 4)
print(f"张量X:\n{X}")
print(f"张量形状: {X.shape}")        # torch.Size([2, 3, 4])
```

## 5. 按元素运算

**概念**: 相同形状的张量可以进行按元素的算术运算，结果张量形状保持不变。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 复制A

# 按元素加法
print(f"按元素加法:\n{A + B}")

# Hadamard积（按元素乘法）
print(f"Hadamard积:\n{A * B}")

# 标量与张量运算
a = 2
print(f"标量乘法形状: {(a * A).shape}")  # torch.Size([5, 4])
```

## 6. 求和与降维

**概念**: 可以对张量的所有元素或沿指定轴进行求和操作，从而降低张量维度。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)

# 全部元素求和
print(f"所有元素求和: {A.sum()}")           # tensor(190.)

# 沿轴0求和（按列求和）
print(f"沿轴0求和: {A.sum(axis=0)}")        # torch.Size([4])

# 沿轴1求和（按行求和）
print(f"沿轴1求和: {A.sum(axis=1)}")        # torch.Size([5])

# 保持维度的求和
sum_A = A.sum(axis=1, keepdims=True)
print(f"保持维度求和形状: {sum_A.shape}")    # torch.Size([5, 1])
```

## 7. 平均值计算

**概念**: 通过将总和除以元素总数来计算平均值，也可以沿指定轴计算。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)

# 计算平均值
print(f"平均值: {A.mean()}")                    # tensor(9.5000)
print(f"验证: {A.sum() / A.numel()}")            # tensor(9.5000)

# 沿轴计算平均值
print(f"沿轴0平均值: {A.mean(axis=0)}")         # torch.Size([4])
```

## 8. 累积求和

**概念**: 沿某个轴计算元素的累积总和，不会降低张量维度。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)

# 沿轴0累积求和
cumsum_result = A.cumsum(axis=0)
print(f"累积求和:\n{cumsum_result}")
print(f"形状保持不变: {cumsum_result.shape}")   # torch.Size([5, 4])
```

## 9. 点积 (Dot Product)

**概念**: 两个向量的点积是相同位置元素乘积的和，表示为x^T y。

**例子**:
```python
x = torch.tensor([0, 1, 2, 3], dtype=torch.float32)
y = torch.ones(4, dtype=torch.float32)

# 点积计算
dot_product = torch.dot(x, y)
print(f"点积: {dot_product}")                    # tensor(6.)

# 等价计算
print(f"等价计算: {torch.sum(x * y)}")           # tensor(6.)
```

## 10. 矩阵-向量积

**概念**: 矩阵A与向量x的乘积，结果是一个向量，每个元素是矩阵对应行与向量的点积。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
x = torch.arange(4, dtype=torch.float32)

# 矩阵-向量积
result = torch.mv(A, x)
print(f"矩阵形状: {A.shape}")        # torch.Size([5, 4])
print(f"向量形状: {x.shape}")        # torch.Size([4])
print(f"结果形状: {result.shape}")   # torch.Size([5])
print(f"结果: {result}")
```

## 11. 矩阵-矩阵乘法

**概念**: 两个矩阵相乘，结果矩阵的每个元素是第一个矩阵对应行与第二个矩阵对应列的点积。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = torch.ones(4, 3)

# 矩阵乘法
C = torch.mm(A, B)
print(f"A形状: {A.shape}")          # torch.Size([5, 4])
print(f"B形状: {B.shape}")          # torch.Size([4, 3])
print(f"结果形状: {C.shape}")       # torch.Size([5, 3])
print(f"结果:\n{C}")
```

## 12. L2范数

**概念**: L2范数是向量元素平方和的平方根，表示向量的欧几里得长度。

**例子**:
```python
u = torch.tensor([3.0, -4.0])

# L2范数计算
l2_norm = torch.norm(u)
print(f"L2范数: {l2_norm}")                      # tensor(5.)

# 手动计算验证
manual_l2 = torch.sqrt(torch.sum(u ** 2))
print(f"手动计算L2范数: {manual_l2}")            # tensor(5.)
```

## 13. L1范数

**概念**: L1范数是向量元素绝对值的和，相比L2范数受异常值影响较小。

**例子**:
```python
u = torch.tensor([3.0, -4.0])

# L1范数计算
l1_norm = torch.abs(u).sum()
print(f"L1范数: {l1_norm}")                      # tensor(7.)

# 使用norm函数计算L1范数
l1_norm_func = torch.norm(u, p=1)
print(f"使用norm函数的L1范数: {l1_norm_func}")   # tensor(7.)
```

## 14. Frobenius范数

**概念**: 矩阵的Frobenius范数是矩阵所有元素平方和的平方根，类似于矩阵的L2范数。

**例子**:
```python
# 创建4x9的全1矩阵
matrix = torch.ones((4, 9))

# Frobenius范数
frobenius_norm = torch.norm(matrix)
print(f"Frobenius范数: {frobenius_norm}")        # tensor(6.)

# 手动计算验证
manual_frob = torch.sqrt(torch.sum(matrix ** 2))
print(f"手动计算: {manual_frob}")                # tensor(6.)
```

## 15. 广播机制

**概念**: 当张量形状兼容时，可以通过广播进行运算，较小的张量会被"扩展"以匹配较大张量的形状。

**例子**:
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_A = A.sum(axis=1, keepdims=True)  # 形状: [5, 1]

# 广播除法
result = A / sum_A
print(f"A形状: {A.shape}")              # torch.Size([5, 4])
print(f"sum_A形状: {sum_A.shape}")      # torch.Size([5, 1])
print(f"广播结果形状: {result.shape}")   # torch.Size([5, 4])
```

## 16. 张量索引和切片

**概念**: 可以使用索引访问张量的特定元素，使用切片访问张量的子集。

**例子**:
```python
X = torch.arange(24).reshape(2, 3, 4)

# 索引访问
print(f"访问[0,1,2]位置的元素: {X[0, 1, 2]}")    # tensor(6)

# 切片访问
print(f"第一个2D切片:\n{X[0]}")
print(f"所有第一列: {X[:, 0, :]}")
print(f"形状: {X[:, 0, :].shape}")              # torch.Size([2, 4])
```

这些知识点构成了深度学习中线性代数的基础，掌握这些操作对于理解和实现神经网络模型至关重要。