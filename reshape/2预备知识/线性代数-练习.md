以下是基于你整理的 **PyTorch版线性代数基础知识** 设计的**测试题目与答案**，涵盖了每个知识点，适合作为练习或测试使用。

---

# 🧪 线性代数基础测试题（PyTorch 版本）

## 📌 一、选择题（每题仅有一个正确答案）

1. 下列哪个不是张量的特例？
   A. 标量
   B. 向量
   C. 矩阵
   D. 字典

2. 若 `x = torch.tensor([1.0, 2.0, 3.0])`，则 `torch.sum(x ** 2)` 的结果是：
   A. 6
   B. 14
   C. 36
   D. 13

3. 若 `A` 为形状 `[5, 4]` 的张量，执行 `A.sum(axis=1, keepdims=True)` 后的结果形状是：
   A. `[5]`
   B. `[4]`
   C. `[5, 4]`
   D. `[5, 1]`

4. 以下哪个范数计算方式能抗异常值影响更强？
   A. L2范数
   B. L1范数
   C. Frobenius范数
   D. 都一样

5. 关于广播机制，以下哪一项是正确的？
   A. 两个张量必须完全形状一致才能计算
   B. 广播不支持张量乘法
   C. 小张量会被扩展以匹配大张量
   D. 只能用于一维张量

---

## 📌 二、判断题（正确请写 True，错误请写 False）

6. `torch.dot(x, y)` 要求 x 和 y 的形状完全一致。

7. 矩阵乘法是将对应元素逐一相乘再求和。

8. `A.mean()` 与 `A.sum() / A.numel()` 计算结果是相等的。

9. `torch.norm(u, p=1)` 可以计算 L1 范数。

10. `torch.mm(A, x)` 可用于矩阵和向量的乘法。

---

## 📌 三、填空题

11. `torch.arange(12).reshape(3, 4)` 创建了一个 \_\_\_\_\_\_\_\_ 形状的张量。

12. 若 `u = torch.tensor([3.0, -4.0])`，则其 L2 范数为 \_\_\_\_\_\_\_\_。

13. 若矩阵 A 形状为 `[3, 2]`，矩阵 B 形状为 `[2, 4]`，则 `torch.mm(A, B)` 的结果形状为 \_\_\_\_\_\_\_\_。

14. `torch.cumsum(A, axis=0)` 表示沿轴 \_\_\_\_\_\_\_\_ 做累积求和。

15. 给定 `X = torch.arange(24).reshape(2, 3, 4)`，访问第一个二维张量的第二行第三列元素，应使用表达式：\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_。

---

## 📌 四、编程题

16. 编写 PyTorch 代码实现：

* 创建一个 `[5, 4]` 的矩阵 A；
* 计算每一行的和并保留维度；
* 对每个元素除以其所在行的总和（广播除法）；
* 打印结果张量的形状。

---

# ✅ 答案与解析

## 一、选择题

1. D ✅（字典不是张量）
2. B ✅（1² + 2² + 3² = 14）
3. D ✅（keepdims=True 保持二维：\[5, 1]）
4. B ✅（L1对异常值更鲁棒）
5. C ✅（广播扩展维度）

---

## 二、判断题

6. True ✅（必须形状相同）
7. False ❌（描述的是 Hadamard 积，矩阵乘法是行与列的点积）
8. True ✅
9. True ✅
10. False ❌（`torch.mv` 用于矩阵-向量，`torch.mm` 用于矩阵-矩阵）

---

## 三、填空题

11. `[3, 4]` ✅
12. `5.0` ✅（√(3² + 4²)）
13. `[3, 4]` ✅
14. `0` ✅
15. `X[0, 1, 2]` ✅

---

## 四、编程题参考答案

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_A = A.sum(axis=1, keepdims=True)  # 保持形状为 [5, 1]
result = A / sum_A  # 广播除法
print(result.shape)  # 应输出 torch.Size([5, 4])
```

输出：

```
torch.Size([5, 4])
```

---

如需更多分级测试题或自动评测脚本，也可以告诉我进一步需求。
